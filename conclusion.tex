\chapter{Conclusion and Further Work}
\label{cha:conclusion}

In this thesis we have investigated several problems related to analysing
distances using techniques known as clustering.  With regards to partitional
clustering we have shown that the problem begins with defining a criterion
with which to determine the suitability of any particular clustering
solution.  We have an intuitive sense of what we'd like to obtain in a
clustering, that is one where the clusters are tightly packed and well
separated, but we have shown that even very similar criteria which have been
used for this purpose can produce wildly different results in the worst case.
We have also shown that the widely used centroid-distance criterion is not
simultaneously a criterion for homogeneity and separation under the
homogeneous Euclidean-overlap metric.  This is contrary to the popular
assumption that it is a criterion for separation when in fact this property
depends on the use of the Euclidean metric.

Even if the sum-of-squares criteria are deemed to be good enough, it is now
known that clustering under the centroid-distance criterion is an NP-hard
problem in Euclidean space.  We have shown that the associated decision
problem is NP-complete even under a simple $p$-valued metric where $p \geq 3$.
However, the problem is solvable in polynomial time for a 2-valued metric.
Similarly, we have shown that the decision problem associated with all-squares
clustering is NP-complete both with Euclidean space and with $p$-valued
metric when $p \geq 3$.  The problem is solvable in polynomial time with a
2-valued metric when the dataset is a set, but the problem remains NP-complete
even for a 2-valued metric when the dataset is a multiset.

In the process of showing the worst case performance of the two sum-of-squares
criteria we have developed a new metric for comparing clusterings called the
assignment metric.  In general this metric allows us to compare sets whose
elements are themselves in a metric space.  This allows us to distinguish
clusterings by using the underlying metric space in a way which is not
possible with other metrics.  It is easily extendable to any metric, as well
as to multiset and fuzzy clusterings.

Our findings reinforce the view that partitional clustering is very difficult
to do precisely.  It is unclear if it even makes sense to try to solve
clustering problems exactly since selection of a criterion is itself so
difficult.  Even so, heuristics for partitional clustering can often do a very
good job when an exact solution to the problem is not required.

Our findings for hierarchical clustering are more positive.  We extended the
problem of simply building a hierarchy of clusters to that of building an
edge-weighted $X$-tree for our dataset $X$.  We then investigated the problem
of building such a tree when only a partial distance on $X$ is given.  The
theory of lassos allows us to decide whether a given set of distances over $X$
is enough to uniquely determine the tree that it they came from and we have
used this information to devise an algorithm for tree reconstruction which
constructs the unique tree for some subset of the given partial distance.



There are still some unsolved problems with regards to the computational
complexity of the partitional clustering problems.  Namely, we wonder whether
it is possible to solve in polynomial time any of the approximation problems
associated to the sum-of-squares optimisation problems or, in other words, can
a near optimal solution be found to within some bounds of the optimal?

It is possible to use the assignment metric with the Hausdorff metric to
``lift'' the underlying metric space up to the space of clusterings.  It is
also possible to use the assignment metric itself for this, but it does incur
a $O(n^5)$ runtime.  It may be possible to reduce this runtime for the special
case of using two (or more) levels of the assignment metric.



%%% Local Variables:
%%% TeX-master: "thesis"
%%% End:
